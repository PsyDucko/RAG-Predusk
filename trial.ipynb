{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bdd4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()   # <-- REQUIRED\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec403d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in .\\venv\\Lib\\site-packages (2.16.0)\n",
      "Requirement already satisfied: chromadb in .\\venv\\Lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: langchain in .\\venv\\Lib\\site-packages (1.2.8)\n",
      "Requirement already satisfied: langchain-openai in .\\venv\\Lib\\site-packages (1.1.7)\n",
      "Requirement already satisfied: langchain-community in .\\venv\\Lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: tiktoken in .\\venv\\Lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dotenv in .\\venv\\Lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: pdfplumber in .\\venv\\Lib\\site-packages (0.11.9)\n",
      "Requirement already satisfied: pypdf in .\\venv\\Lib\\site-packages (6.6.2)\n",
      "Requirement already satisfied: pdf2image in .\\venv\\Lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pytesseract in .\\venv\\Lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pillow in .\\venv\\Lib\\site-packages (12.1.0)\n",
      "Requirement already satisfied: pandas in .\\venv\\Lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in .\\venv\\Lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: tabulate in .\\venv\\Lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in .\\venv\\Lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in .\\venv\\Lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\venv\\Lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in .\\venv\\Lib\\site-packages (from openai) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in .\\venv\\Lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in .\\venv\\Lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in .\\venv\\Lib\\site-packages (from openai) (4.67.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in .\\venv\\Lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in .\\venv\\Lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in .\\venv\\Lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in .\\venv\\Lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in .\\venv\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in .\\venv\\Lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in .\\venv\\Lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in .\\venv\\Lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: build>=1.0.3 in .\\venv\\Lib\\site-packages (from chromadb) (1.4.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in .\\venv\\Lib\\site-packages (from chromadb) (1.4.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in .\\venv\\Lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in .\\venv\\Lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in .\\venv\\Lib\\site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in .\\venv\\Lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in .\\venv\\Lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in .\\venv\\Lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in .\\venv\\Lib\\site-packages (from chromadb) (0.22.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in .\\venv\\Lib\\site-packages (from chromadb) (0.51.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in .\\venv\\Lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in .\\venv\\Lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in .\\venv\\Lib\\site-packages (from chromadb) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in .\\venv\\Lib\\site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in .\\venv\\Lib\\site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in .\\venv\\Lib\\site-packages (from chromadb) (35.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in .\\venv\\Lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in .\\venv\\Lib\\site-packages (from chromadb) (6.0.3)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in .\\venv\\Lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in .\\venv\\Lib\\site-packages (from chromadb) (3.11.7)\n",
      "Requirement already satisfied: rich>=10.11.0 in .\\venv\\Lib\\site-packages (from chromadb) (14.3.2)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in .\\venv\\Lib\\site-packages (from chromadb) (4.26.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in .\\venv\\Lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in .\\venv\\Lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in .\\venv\\Lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in .\\venv\\Lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in .\\venv\\Lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\venv\\Lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2.6.3)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.8 in .\\venv\\Lib\\site-packages (from langchain) (1.2.8)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in .\\venv\\Lib\\site-packages (from langchain) (1.0.7)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (0.6.8)\n",
      "Requirement already satisfied: packaging>=23.2.0 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (26.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in .\\venv\\Lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.8->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in .\\venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in .\\venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in .\\venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in .\\venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in .\\venv\\Lib\\site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in .\\venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in .\\venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (0.25.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in .\\venv\\Lib\\site-packages (from tiktoken) (2026.1.15)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in .\\venv\\Lib\\site-packages (from langchain-community) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in .\\venv\\Lib\\site-packages (from langchain-community) (2.0.46)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in .\\venv\\Lib\\site-packages (from langchain-community) (3.13.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in .\\venv\\Lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in .\\venv\\Lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in .\\venv\\Lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in .\\venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in .\\venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in .\\venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in .\\venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in .\\venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in .\\venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in .\\venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in .\\venv\\Lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in .\\venv\\Lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in .\\venv\\Lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: greenlet>=1 in .\\venv\\Lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in .\\venv\\Lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: pdfminer.six==20251230 in .\\venv\\Lib\\site-packages (from pdfplumber) (20251230)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in .\\venv\\Lib\\site-packages (from pdfplumber) (5.3.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in .\\venv\\Lib\\site-packages (from pdfminer.six==20251230->pdfplumber) (46.0.4)\n",
      "Requirement already satisfied: tzdata in .\\venv\\Lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: pyproject_hooks in .\\venv\\Lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in .\\venv\\Lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: cffi>=2.0.0 in .\\venv\\Lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in .\\venv\\Lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in .\\venv\\Lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in .\\venv\\Lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in .\\venv\\Lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in .\\venv\\Lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in .\\venv\\Lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in .\\venv\\Lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: coloredlogs in .\\venv\\Lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in .\\venv\\Lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
      "Requirement already satisfied: protobuf in .\\venv\\Lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.33.5)\n",
      "Requirement already satisfied: sympy in .\\venv\\Lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in .\\venv\\Lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
      "Requirement already satisfied: zipp>=3.20 in .\\venv\\Lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in .\\venv\\Lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in .\\venv\\Lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.1 in .\\venv\\Lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in .\\venv\\Lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in .\\venv\\Lib\\site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in .\\venv\\Lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in .\\venv\\Lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in .\\venv\\Lib\\site-packages (from tokenizers>=0.13.2->chromadb) (1.3.7)\n",
      "Requirement already satisfied: filelock in .\\venv\\Lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\venv\\Lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in .\\venv\\Lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
      "Requirement already satisfied: shellingham in .\\venv\\Lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in .\\venv\\Lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (0.21.1)\n",
      "Requirement already satisfied: click>=8.0.0 in .\\venv\\Lib\\site-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
      "Requirement already satisfied: httptools>=0.6.3 in .\\venv\\Lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in .\\venv\\Lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in .\\venv\\Lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (16.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in .\\venv\\Lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in .\\venv\\Lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in .\\venv\\Lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in .\\venv\\Lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai chromadb langchain langchain-openai langchain-community tiktoken python-dotenv \\\n",
    "    pdfplumber pypdf pdf2image pytesseract pillow pandas numpy tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8f3bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in .\\venv\\Lib\\site-packages (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ed04b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_text_splitters in .\\venv\\Lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in .\\venv\\Lib\\site-packages (from langchain_text_splitters) (1.2.8)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.6.8)\n",
      "Requirement already satisfied: packaging>=23.2.0 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (26.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in .\\venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in .\\venv\\Lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in .\\venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in .\\venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in .\\venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (2.32.5)\n",
      "Requirement already satisfied: xxhash>=3.0.0 in .\\venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (3.6.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in .\\venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.25.0)\n",
      "Requirement already satisfied: anyio in .\\venv\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (4.12.1)\n",
      "Requirement already satisfied: certifi in .\\venv\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in .\\venv\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in .\\venv\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in .\\venv\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in .\\venv\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in .\\venv\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in .\\venv\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in .\\venv\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\venv\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (2.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f37c85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# PDF processing\n",
    "import pdfplumber\n",
    "from pypdf import PdfReader\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load environment variables\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()   \n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a7da1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b8225c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PDFContentExtractor class defined\n"
     ]
    }
   ],
   "source": [
    "class PDFContentExtractor:\n",
    "    \"\"\"\n",
    "    Comprehensive PDF content extractor that handles:\n",
    "    - Text extraction\n",
    "    - Table extraction and conversion to text\n",
    "    - Image extraction and OCR\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pdf_path: str, extract_images: bool = True, ocr: bool = False):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.extract_images = extract_images\n",
    "        self.ocr = ocr\n",
    "        self.pages_content = []\n",
    "        self.extracted_images = []\n",
    "        self.extracted_tables = []\n",
    "    \n",
    "    def extract_all(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract all content from PDF\n",
    "        \"\"\"\n",
    "        print(f\"Processing PDF: {self.pdf_path}\")\n",
    "        \n",
    "        with pdfplumber.open(self.pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                print(f\"  Processing page {page_num}/{len(pdf.pages)}...\")\n",
    "                \n",
    "                page_data = {\n",
    "                    'page_number': page_num,\n",
    "                    'text': '',\n",
    "                    'tables': [],\n",
    "                    'images': []\n",
    "                }\n",
    "                \n",
    "                # Extract text\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    page_data['text'] = text.strip()\n",
    "                \n",
    "                # Extract tables\n",
    "                tables = page.extract_tables()\n",
    "                for table_idx, table in enumerate(tables):\n",
    "                    if table:\n",
    "                        table_text = self._table_to_text(table, page_num, table_idx)\n",
    "                        page_data['tables'].append(table_text)\n",
    "                        self.extracted_tables.append({\n",
    "                            'page': page_num,\n",
    "                            'table_index': table_idx,\n",
    "                            'data': table,\n",
    "                            'text': table_text\n",
    "                        })\n",
    "                \n",
    "                self.pages_content.append(page_data)\n",
    "        \n",
    "        # Extract images if requested\n",
    "        if self.extract_images:\n",
    "            self._extract_images()\n",
    "        \n",
    "        return self._compile_results()\n",
    "    \n",
    "    def _table_to_text(self, table: List[List], page_num: int, table_idx: int) -> str:\n",
    "        \"\"\"\n",
    "        Convert table to readable text format\n",
    "        \"\"\"\n",
    "        if not table or len(table) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        # Try to identify header row\n",
    "        header = table[0] if table else []\n",
    "        rows = table[1:] if len(table) > 1 else []\n",
    "        \n",
    "        # Create text representation\n",
    "        text_parts = [f\"\\n[Table {table_idx + 1} on Page {page_num}]\\n\"]\n",
    "        \n",
    "        # Add header\n",
    "        if header:\n",
    "            text_parts.append(\" | \".join(str(cell) if cell else \"\" for cell in header))\n",
    "            text_parts.append(\"-\" * 80)\n",
    "        \n",
    "        # Add rows\n",
    "        for row in rows:\n",
    "            text_parts.append(\" | \".join(str(cell) if cell else \"\" for cell in row))\n",
    "        \n",
    "        return \"\\n\".join(text_parts)\n",
    "    \n",
    "    def _extract_images(self):\n",
    "        \"\"\"\n",
    "        Extract images from PDF pages\n",
    "        \"\"\"\n",
    "        print(\"  Extracting images from PDF...\")\n",
    "        \n",
    "        try:\n",
    "            # Convert PDF pages to images\n",
    "            images = convert_from_path(self.pdf_path, dpi=150)\n",
    "            \n",
    "            for page_num, image in enumerate(images, 1):\n",
    "                # Save image info\n",
    "                img_data = {\n",
    "                    'page': page_num,\n",
    "                    'image': image,\n",
    "                    'path': None  # Can save to disk if needed\n",
    "                }\n",
    "                \n",
    "                # Perform OCR if requested\n",
    "                if self.ocr:\n",
    "                    try:\n",
    "                        import pytesseract\n",
    "                        ocr_text = pytesseract.image_to_string(image)\n",
    "                        img_data['ocr_text'] = ocr_text\n",
    "                    except Exception as e:\n",
    "                        print(f\"    OCR failed for page {page_num}: {e}\")\n",
    "                \n",
    "                self.extracted_images.append(img_data)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Image extraction failed: {e}\")\n",
    "    \n",
    "    def _compile_results(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Compile all extracted content\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'pages': self.pages_content,\n",
    "            'total_pages': len(self.pages_content),\n",
    "            'total_tables': len(self.extracted_tables),\n",
    "            'total_images': len(self.extracted_images),\n",
    "            'tables': self.extracted_tables,\n",
    "            'images': self.extracted_images\n",
    "        }\n",
    "    \n",
    "    def create_documents(self) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Create LangChain Document objects from extracted content\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for page_data in self.pages_content:\n",
    "            page_num = page_data['page_number']\n",
    "            \n",
    "            # Combine text and tables for the page\n",
    "            content_parts = []\n",
    "            \n",
    "            if page_data['text']:\n",
    "                content_parts.append(page_data['text'])\n",
    "            \n",
    "            if page_data['tables']:\n",
    "                content_parts.extend(page_data['tables'])\n",
    "            \n",
    "            # Add OCR text from images if available\n",
    "            for img in self.extracted_images:\n",
    "                if img['page'] == page_num and 'ocr_text' in img and img['ocr_text']:\n",
    "                    content_parts.append(f\"\\n[Image OCR Text]\\n{img['ocr_text']}\")\n",
    "            \n",
    "            if content_parts:\n",
    "                combined_content = \"\\n\\n\".join(content_parts)\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=combined_content,\n",
    "                    metadata={\n",
    "                        'source': self.pdf_path,\n",
    "                        'page': page_num,\n",
    "                        'has_tables': len(page_data['tables']) > 0,\n",
    "                        'num_tables': len(page_data['tables'])\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        \n",
    "        return documents\n",
    "\n",
    "print(\"✓ PDFContentExtractor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16ae6680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: C:\\Users\\ASUS\\OneDrive\\Desktop\\RAG\\pdfs\\Paradigms_of_Programming.pdf\n",
      "================================================================================\n",
      "Processing PDF: C:\\Users\\ASUS\\OneDrive\\Desktop\\RAG\\pdfs\\Paradigms_of_Programming.pdf\n",
      "  Processing page 1/46...\n",
      "  Processing page 2/46...\n",
      "  Processing page 3/46...\n",
      "  Processing page 4/46...\n",
      "  Processing page 5/46...\n",
      "  Processing page 6/46...\n",
      "  Processing page 7/46...\n",
      "  Processing page 8/46...\n",
      "  Processing page 9/46...\n",
      "  Processing page 10/46...\n",
      "  Processing page 11/46...\n",
      "  Processing page 12/46...\n",
      "  Processing page 13/46...\n",
      "  Processing page 14/46...\n",
      "  Processing page 15/46...\n",
      "  Processing page 16/46...\n",
      "  Processing page 17/46...\n",
      "  Processing page 18/46...\n",
      "  Processing page 19/46...\n",
      "  Processing page 20/46...\n",
      "  Processing page 21/46...\n",
      "  Processing page 22/46...\n",
      "  Processing page 23/46...\n",
      "  Processing page 24/46...\n",
      "  Processing page 25/46...\n",
      "  Processing page 26/46...\n",
      "  Processing page 27/46...\n",
      "  Processing page 28/46...\n",
      "  Processing page 29/46...\n",
      "  Processing page 30/46...\n",
      "  Processing page 31/46...\n",
      "  Processing page 32/46...\n",
      "  Processing page 33/46...\n",
      "  Processing page 34/46...\n",
      "  Processing page 35/46...\n",
      "  Processing page 36/46...\n",
      "  Processing page 37/46...\n",
      "  Processing page 38/46...\n",
      "  Processing page 39/46...\n",
      "  Processing page 40/46...\n",
      "  Processing page 41/46...\n",
      "  Processing page 42/46...\n",
      "  Processing page 43/46...\n",
      "  Processing page 44/46...\n",
      "  Processing page 45/46...\n",
      "  Processing page 46/46...\n",
      "  Extracting images from PDF...\n",
      "  Image extraction failed: Unable to get page count. Is poppler installed and in PATH?\n",
      "\n",
      "Extracted:\n",
      "  - 46 pages\n",
      "  - 46 tables\n",
      "  - 0 images\n",
      "  - Created 46 document chunks\n",
      "\n",
      "✓ Total documents to process: 46\n"
     ]
    }
   ],
   "source": [
    "# Example: Process your PDF files\n",
    "# Replace with your actual PDF paths\n",
    "\n",
    "pdf_files = [\n",
    "    r\"C:\\Users\\ASUS\\OneDrive\\Desktop\\RAG\\pdfs\\Paradigms_of_Programming.pdf\",\n",
    "]\n",
    "\n",
    "# For demonstration, we'll create a sample PDF (optional)\n",
    "# You can skip this if you have your own PDFs\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "if pdf_files:\n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_path}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        extractor = PDFContentExtractor(\n",
    "            pdf_path=pdf_path,\n",
    "            extract_images=True,  # Extract images\n",
    "            ocr=False  # Set to True if you want OCR on images\n",
    "        )\n",
    "        \n",
    "        # Extract all content\n",
    "        results = extractor.extract_all()\n",
    "        \n",
    "        print(f\"\\nExtracted:\")\n",
    "        print(f\"  - {results['total_pages']} pages\")\n",
    "        print(f\"  - {results['total_tables']} tables\")\n",
    "        print(f\"  - {results['total_images']} images\")\n",
    "        \n",
    "        # Create documents\n",
    "        docs = extractor.create_documents()\n",
    "        all_documents.extend(docs)\n",
    "        \n",
    "        print(f\"  - Created {len(docs)} document chunks\")\n",
    "else:\n",
    "    print(\"No PDF files specified. Please add your PDF file paths to the 'pdf_files' list above.\")\n",
    "    print(\"\\nUsing sample text documents for demonstration...\")\n",
    "    \n",
    "    # Sample documents for testing\n",
    "    sample_texts = [\n",
    "        \"\"\"Machine Learning is a subset of AI that focuses on algorithms that can learn from data.\n",
    "        It includes supervised learning, unsupervised learning, and reinforcement learning.\"\"\",\n",
    "        \n",
    "        \"\"\"Deep Learning uses neural networks with multiple layers to learn hierarchical representations.\n",
    "        It has achieved remarkable success in image recognition and natural language processing.\"\"\",\n",
    "    ]\n",
    "    \n",
    "    all_documents = [Document(page_content=text, metadata={\"source\": \"sample\", \"page\": i}) \n",
    "                     for i, text in enumerate(sample_texts, 1)]\n",
    "\n",
    "print(f\"\\n✓ Total documents to process: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1764c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Split into 78 chunks\n",
      "\n",
      "Sample chunk:\n",
      "--------------------------------------------------------------------------------\n",
      "GettingStarted\n",
      "Black-BoxAbstraction\n",
      "Foodforthought\n",
      "Welcome to CS302\n",
      "Paradigms of Programming\n",
      "1/28\n",
      "\n",
      "\n",
      "[Table 1 on Page 1]\n",
      "\n",
      "GettingStarted\n",
      "Black-BoxAbstraction\n",
      "Foodforthought | \n",
      "--------------------------------------------------------------------------------\n",
      "Welcome to CS302\n",
      "Paradigms of Programming\n",
      "1/28 |\n",
      "\n",
      "Metadata: {'source': 'C:\\\\Users\\\\ASUS\\\\OneDrive\\\\Desktop\\\\RAG\\\\pdfs\\\\Paradigms_of_Programming.pdf', 'page': 1, 'has_tables': True, 'num_tables': 1}\n"
     ]
    }
   ],
   "source": [
    "# Configure text splitter for optimal chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Larger chunks to preserve table context\n",
    "    chunk_overlap=200,  # More overlap to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "splits = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"✓ Split into {len(splits)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(\"-\" * 80)\n",
    "if splits:\n",
    "    print(splits[0].page_content[:500] + \"...\" if len(splits[0].page_content) > 500 else splits[0].page_content)\n",
    "    print(f\"\\nMetadata: {splits[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b6e3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295b1101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating vector store...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Create vector store with persistence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m vectorstore = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpdf_rag_collection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./chroma_pdf_db\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Vector store created and persisted to ./chroma_pdf_db\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\OneDrive\\Desktop\\RAG\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:887\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    885\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    886\u001b[39m metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\OneDrive\\Desktop\\RAG\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m    838\u001b[39m         api=chroma_collection._client,\n\u001b[32m    839\u001b[39m         ids=ids,\n\u001b[32m    840\u001b[39m         metadatas=metadatas,\n\u001b[32m    841\u001b[39m         documents=texts,\n\u001b[32m    842\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\OneDrive\\Desktop\\RAG\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    281\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\OneDrive\\Desktop\\RAG\\venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:709\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;66;03m# Unconditionally call _get_len_safe_embeddings to handle length safety.\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[38;5;66;03m# This could be optimized to avoid double work when all texts are short enough.\u001b[39;00m\n\u001b[32m    708\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\OneDrive\\Desktop\\RAG\\venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:576\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# Make API call with this batch\u001b[39;00m\n\u001b[32m    575\u001b[39m batch_tokens = tokens[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    578\u001b[39m     response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\OneDrive\\Desktop\\RAG\\venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\OneDrive\\Desktop\\RAG\\venv\\Lib\\site-packages\\openai\\_base_client.py:1294\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1285\u001b[39m     warnings.warn(\n\u001b[32m   1286\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1287\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1288\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1289\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1290\u001b[39m     )\n\u001b[32m   1291\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1292\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1293\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\OneDrive\\Desktop\\RAG\\venv\\Lib\\site-packages\\openai\\_base_client.py:1067\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1064\u001b[39m             err.response.read()\n\u001b[32m   1066\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # Fast and efficient\n",
    "    # model=\"text-embedding-3-large\",  # Use this for higher quality\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "print(\"Creating vector store...\")\n",
    "\n",
    "# Create vector store with persistence\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"pdf_rag_collection\",\n",
    "    persist_directory=\"./chroma_pdf_db\"\n",
    ")\n",
    "\n",
    "print(\"✓ Vector store created and persisted to ./chroma_pdf_db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
